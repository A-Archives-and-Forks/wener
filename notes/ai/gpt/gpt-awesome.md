---
tags:
  - Awesome
---

# GPT Awesome

- GPT - Generative Pre-trained Transformer - 生成型预训练变换模型
- wikipedia [GPT-3](https://en.wikipedia.org/wiki/GPT-3)
  - autoregressive language model - 自回归语言模型
- [openai/gpt-3](https://github.com/openai/gpt-3)
- [yandex/YaLM-100B](https://github.com/yandex/YaLM-100B)
  - GPT Pretrained language model with 100B parameters
  - [HN](https://news.ycombinator.com/item?id=31846593)
- [EssayKillerBrain/EssayKiller_V2](https://github.com/EssayKillerBrain/EssayKiller_V2)
- [BeautyYuYanli/full-mark-composition-generator](https://github.com/BeautyYuYanli/full-mark-composition-generator)
- [menzi11/BullshitGenerator](https://github.com/menzi11/BullshitGenerator)
- [suulnnka/BullshitGenerator](https://github.com/suulnnka/BullshitGenerator)
  - https://suulnnka.github.io/BullshitGenerator/index.html
- [Torantulino/Auto-GPT](https://github.com/Torantulino/Auto-GPT)
- [PromtEngineer/localGPT](https://github.com/PromtEngineer/localGPT)
- OpenAI
  - https://gptstore.ai/
  - https://github.com/linexjlin/GPTs
  - https://quail.ink/goldengrape/p/how-to-build-a-patent-gpt
- https://nihalsid.github.io/mesh-gpt/
  - https://news.ycombinator.com/item?id=38448653
- [GPT in 500 Lines of SQL ](https://explainextended.com/2023/12/31/happy-new-year-15/)
  - https://news.ycombinator.com/item?id=39488668

### Reading

- What is GPT-3? written in layman's terms
  - https://news.ycombinator.com/item?id=23923799
- [GPT-J is self-hosted open-source analog of GPT-3](https://tracklify.com/blog/gpt-j-is-self-hosted-open-source-analog-of-gpt-3-how-to-run-in-docker/)

# FAQ

## Hallucination

- 幻觉
- 很真实，当不是真正的正确的结果
- 要求给相关信息，然后基于相关信息再回答。
